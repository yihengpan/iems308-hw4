{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import Query_generation as Qg\n",
    "import QuestionClassifier as QC\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.chunk import *\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "import math as m\n",
    "from fractions import Fraction\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document_set = Qg.Document_subset(Qg.ans,20)\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rule_Based_AnswerSystem(Question):\n",
    "    Question_class = QC.test_questions(Question)\n",
    "    rule = {'HUM':'PERSON','LOC':'GPE','ENTY':['NNP','NN','NNS'],'DESC':['NNP','NN','NNS'],'NUM':'CD','ABBR':['NNP','NN','NNS']}\n",
    "    Ans_class = rule[Question_class]\n",
    "    \n",
    "    return Ans_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAnsByLabel(Document_set,Ans_class):\n",
    "    Ans_Back_up = []\n",
    "    for Document in Document_set:\n",
    "        file = open(Document)\n",
    "        for row in file:\n",
    "            row = row.replace('\\n','')\n",
    "            row = row.replace('\\r','')\n",
    "            try:\n",
    "                sentence = sent_tokenize(row)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            for singleSentence in sentence:\n",
    "                flag = False\n",
    "                words = singleSentence.split()\n",
    "                for word in words:\n",
    "                    if Query_term.has_key(word) and flag == False:\n",
    "                        Ans_Back_up.append(singleSentence)\n",
    "                        flag = True\n",
    "                        \n",
    "    final_ans = []\n",
    "    for sentence in Ans_Back_up:\n",
    "        flag = False\n",
    "        token = nltk.word_tokenize(sentence)\n",
    "        pos_tags = nltk.pos_tag(token)\n",
    "        result_ = nltk.ne_chunk(pos_tags,binary = False)\n",
    "        \n",
    "        for item in result_:\n",
    "            try:\n",
    "                if item.label() == Ans_class and flag == False:\n",
    "                    final_ans.append(sentence)\n",
    "                    flag = True\n",
    "                    \n",
    "            except:\n",
    "                if flag == False:\n",
    "                    if len(Ans_class) == 3:\n",
    "                        inerflag = False\n",
    "                        for tag in pos_tags:\n",
    "                            if tag[1] == 'CD':\n",
    "                                inerflag = True\n",
    "                        if inerflag == False:\n",
    "                            final_ans.append(sentence)\n",
    "                            flag = True\n",
    "                    elif Ans_class == 'CD':\n",
    "                        if item[1] == 'CD':\n",
    "                            final_ans.append(sentence)\n",
    "                            flag = True\n",
    "    return final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(sentence):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    Q = nltk.word_tokenize(sentence)\n",
    "    result = []\n",
    "    result.extend([i for i in Q if i not in stop])\n",
    "    result_bigram = list(nltk.bigrams(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Score_System(Ans_set,question_):\n",
    "    question = nltk.word_tokenize(question_)\n",
    "    pattern = r'[^A-Za-z0-9\\s]+'\n",
    "    question = re.sub(pattern,'',question_)\n",
    "    question_bigram = list(nltk.bigrams(question))\n",
    "    Number_of_Same_word = 0\n",
    "    total_score = {}\n",
    "    for sentence in Ans_set:\n",
    "        label = sentence\n",
    "        score = 0\n",
    "        sentence = nltk.word_tokenize(sentence)\n",
    "        sentence_bigram = list(nltk.bigrams(sentence))\n",
    "        #score of bigram match\n",
    "        for bigram in question_bigram:\n",
    "            for item in sentence_bigram:\n",
    "                if item == bigram:\n",
    "                    score = score + 2\n",
    "        #score of unigram match\n",
    "        for item in question:\n",
    "            for word in sentence:\n",
    "                if word == item:\n",
    "                    score = score + 1\n",
    "                    \n",
    "        score = Fraction(score,len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_TDF(keywords, Relevant_Documents, total_Counts):\n",
    "    qtf = keywords\n",
    "    df = Doc_Freq(keywords, Relevant_Documents)\n",
    "    tf_lists = {}\n",
    "    for document in Relevant_Documents:\n",
    "        tf = Term_Freq(keywords, document)\n",
    "        if tf != {}:\n",
    "            tf_lists[document] = tf\n",
    "            \n",
    "    TF_IDF = {}\n",
    "    for key, val in tf_lists.iterirems():\n",
    "        score = 0\n",
    "        term = val\n",
    "        max_term = MaxTerm(key)\n",
    "        for eachterm, value in term.iteritems():\n",
    "            TF = 0.5 + (0.5 * value / max_term)\n",
    "            IDF = m.log(Fraction(total_Counts, df[eachterm]))\n",
    "            \n",
    "            tf_idf = TF * IDF\n",
    "            score = score + tf_idf\n",
    "            \n",
    "        if score != 0:\n",
    "            TF_IDF[key] = score\n",
    "            \n",
    "    return sorted(TF_IDF.iteritems(), key = operator.itemgetter(1), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Term_Select(question):\n",
    "    stop = set(stopwords.word('english'))\n",
    "    regex = r'[^A-Za-z0-9\\s]+'\n",
    "    question = re.sub(regex, '', question)\n",
    "    tokenwords = nltk.word_tokenize(question)\n",
    "    lists = []\n",
    "    for word in tokenwords:\n",
    "        if word not in stop:\n",
    "            lists.append(word)\n",
    "            \n",
    "    pos_tags = nltk.pos_tag(lists)\n",
    "    Query_Terms = {}\n",
    "    for item in pos_tags:\n",
    "        if item[0] != 'What' and item[0] != 'Which' and item[0] != 'When' and item[0] != 'Where' and item[0] != 'Whose' and item[0] != 'Who' and item[0] != 'How' and not Query_Terms.has_key(item[0]):\n",
    "            Query_Terms[item[0]] = 1\n",
    "        elif item[0] != 'What' and item[0] != 'Which' and item[0] != 'When' and item[0] != 'Where' and item[0] != 'Whose' and item[0] != 'Who' and item[0] != 'How' and Query_Terms.has_key(item[0]):\n",
    "            Query_Terms[item[0]] = Query_Terms[item[0]] + 1\n",
    "            \n",
    "    return Query_Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxTerm(Document):\n",
    "    file = open(Document)\n",
    "    MaximumTerms = {}\n",
    "    stop = set(stopwords.words('English'))\n",
    "    for row in file:\n",
    "        row = row.replace('\\n','')\n",
    "        words = row.split(' ')\n",
    "        for word in words:\n",
    "            if word not in stop and word != 'I' and word != 'The' and word != 'would' and word != 'Would':\n",
    "                if not MaximumTerms.has_key(word):\n",
    "                    MaximumTerms[word] = 1\n",
    "                else:\n",
    "                    MaximumTerms[word] = MaximumTerms[word] + 1\n",
    "    return max(MaximumTerms.iteritems(), key=operator.itemgetter(1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Term_frequency(queryterm,sentence):\n",
    "    tf = dict.fromkeys(queryterm.keys(),0)\n",
    "    for word in sentence:\n",
    "        if queryterm.has_key(word):\n",
    "            tf[word] = tf[word] + 1\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_Term(question):\n",
    "    question = question.replace('?','')\n",
    "    question_ = nltk.word_tokenize(question)\n",
    "    pos_tags = nltk.pos_tag(question_)\n",
    "    question = []\n",
    "    for item in pos_tags:\n",
    "        if item[0] != 'Which':\n",
    "            if item[1] == 'NNP' or item[1] == 'NNS' or item[1] == 'NN' or item[1] == 'CD' or item[1] == 'JJ':\n",
    "                question.append(item[0])\n",
    "\n",
    "    question_term = {}\n",
    "#create question term\n",
    "\n",
    "    for item in question:\n",
    "\n",
    "        if not question_term.has_key(item):\n",
    "            question_term[item] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            question_term[item] = question_term[item] + 1\n",
    "\n",
    "    return question_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query_term = dict.fromkeys(Qg.queryterms.keys(),0)\n",
    "Ans_type = Rule_Based_AnswerSystem(Qg.question)\n",
    "\n",
    "ansset = filterAnsByLabel(Document_set,Ans_type)\n",
    "# ansset = Document_set\n",
    "# print Answer_Score_System(ansset,Qg.question)\n",
    "ans = TF_IDF(ansset,Qg.question)\n",
    "ans_sub = Document_subset(ans,10)\n",
    "print ans_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
